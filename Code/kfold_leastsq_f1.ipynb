{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "print(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=2020)\n",
    "print(X_train, X_test, y_train, y_test)\n",
    "from sklearn.model_selection import KFold\n",
    "X, y = np.arange(18).reshape((9, 2)), np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "kf = KFold(n_splits=3)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leastsq\n",
    "# 1 ###最小二乘法试验###\n",
    "# 1、p0里放的是k、b的初始值，这个值可以随意指定。往后随着迭代次数增加，k、b将会不断变化，使得error函数的值越来越小。\n",
    "# 2、func函数里指出了待拟合函数的函数形状。\n",
    "# 3、error函数为误差函数，我们的目标就是不断调整k和b使得error不断减小。这里的error函数和神经网络中常说的cost函数实际上是一回事，只不过这里更简单些而已。\n",
    "# 4、必须注意一点，传入leastsq函数的参数可以有多个，但必须把参数的初始值p0和其它参数分开放。其它参数应打包到args中。\n",
    "# 5、leastsq的返回值是一个tuple，它里面有两个元素，第一个元素是k、b的求解结果，第二个元素我暂时也不知道是什么意思，先留下来。\n",
    "#  2 import numpy as np\n",
    "#  3 from scipy.optimize import leastsq\n",
    "#  4\n",
    "#  5 ###采样点(Xi,Yi)###\n",
    "#  6 Xi=np.array([8.19,2.72,6.39,8.71,4.7,2.66,3.78])\n",
    "#  7 Yi=np.array([7.01,2.78,6.47,6.71,4.1,4.23,4.05])\n",
    "#  8\n",
    "#  9 ###需要拟合的函数func及误差error###\n",
    "# 10 def func(p,x):\n",
    "# 11     k,b=p\n",
    "# 12     return k*x+b\n",
    "# 13\n",
    "# 14 def error(p,x,y,s):\n",
    "# 15     print s\n",
    "# 16     return func(p,x)-y #x、y都是列表，故返回值也是个列表\n",
    "# 17\n",
    "# 18 #TEST\n",
    "# 19 p0=[100,2]\n",
    "# 20 #print( error(p0,Xi,Yi) )\n",
    "# 21\n",
    "# 22 ###主函数从此开始###\n",
    "# 23 s=\"Test the number of iteration\" #试验最小二乘法函数leastsq得调用几次error函数才能找到使得均方误差之和最小的k、b\n",
    "# 24 Para=leastsq(error,p0,args=(Xi,Yi,s)) #把error函数中除了p以外的参数打包到args中\n",
    "# 25 k,b=Para[0]\n",
    "# 26 print\"k=\",k,'\\n',\"b=\",b\n",
    "# 27\n",
    "# 28 ###绘图，看拟合效果###\n",
    "# 29 import matplotlib.pyplot as plt\n",
    "# 30\n",
    "# 31 plt.figure(figsize=(8,6))\n",
    "# 32 plt.scatter(Xi,Yi,color=\"red\",label=\"Sample Point\",linewidth=3) #画样本点\n",
    "# 33 x=np.linspace(0,10,1000)\n",
    "# 34 y=k*x+b\n",
    "# 35 plt.plot(x,y,color=\"orange\",label=\"Fitting Line\",linewidth=2) #画拟合直线\n",
    "# 36 plt.legend()\n",
    "# 37 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有数据的F1-score:\n",
    "# 有两种方式\n",
    "# 第一种方式是计算数据中所有的TP,FP,FN,然后计算F1-score,即micro；\n",
    "# 第二种方式是分别计算各个类别的TP，FP，FN，然后计算各个类被的F1-score,然后对F-score求平均，即macro.\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "y_true = [1, 1, 1, 1, 2, 2, 2, 3, 3]\n",
    "y_pred = [1, 1, 2, 3, 2, 2, 3, 2, 3]\n",
    " \n",
    "f1_micro = f1_score(y_true,y_pred,average='micro')\n",
    "f1_macro = f1_score(y_true,y_pred,average='macro')\n",
    " \n",
    " \n",
    "print('f1_micro: {0}'.format(f1_micro))\n",
    "print('f1_macro: {0}'.format(f1_macro))\n",
    "# 计算各个类别的准确率，召回率，与F1-score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    " \n",
    "y_true = [1, 1, 1, 1, 2, 2, 2, 3, 3]\n",
    "y_pred = [1, 1, 2, 3, 2, 2, 3, 2, 3]\n",
    " \n",
    "p_class, r_class, f_class, support_micro = precision_recall_fscore_support(y_true,y_pred,labels=[1,2,3])\n",
    " \n",
    "print(p_class)\n",
    "print(r_class)\n",
    "print(f_class)\n",
    "print(support_micro)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
