{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多层感知机的从0开始实现\n",
    "import torch\n",
    "import numpy as np\n",
    "import d2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2p' has no attribute 'load_data_fashion_mnist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-33f88259e72a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md2p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#定义模型参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'd2p' has no attribute 'load_data_fashion_mnist'"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=d2p.load_data_fashion_mnist(batch_size)\n",
    "#定义模型参数\n",
    "num_inputs,num_outputs,num_hiddens=784,10,256\n",
    "W1=torch.tensor(np.random.normal(0,0.01,(num_inputs,num_hiddens)),dtype=torch.float)\n",
    "W2=torch.tensor(np.random.normal(0,0.01,(num_hiddens,num_outputs)),dtype=torch.float)\n",
    "b1=torch.zeros(num_hiddens,dtype=torch.float)\n",
    "b2=torch.zeros(num_outputs,dtype=torch.float)\n",
    "params = [W1,b1,W2,b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)\n",
    "\n",
    "#定义激活函数\n",
    "def relu(X):\n",
    "    return torch.max(input=X,other=torch.tensor(0.0))\n",
    "#定义模型\n",
    "def net(X):\n",
    "    X=X.view((-1,num_inputs))\n",
    "    H=relu(torch.matmul(X,W1)+b1)\n",
    "    return torch.matmul(H,W2)+b2\n",
    "#定义损失函数\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "#训练模型\n",
    "num_epochs, lr = 5, 100.0\n",
    "d2p.train_ch3(net, train_iter, test_iter, loss, num_epochs,batch_size, params, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#只对权重衰减，不对偏差衰减\n",
    "def fit_and_plot_pytorch(wd):\n",
    "    # 对权重参数衰减。权重名称⼀一般是以weight结尾\n",
    "    net = nn.Linear(num_inputs, 1)\n",
    "    nn.init.normal_(net.weight, mean=0, std=1)\n",
    "    nn.init.normal_(net.bias, mean=0, std=1)\n",
    "    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr,\n",
    "    weight_decay=wd) # 对权重参数衰减\n",
    "    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr) \n",
    "    # 不对偏差参数衰减\n",
    "    train_ls, ltest_ls = [], []\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            l = loss(net(X), y).mean()\n",
    "            optimizer_w.zero_grad()\n",
    "            optimizer_b.zero_grad()\n",
    "            l.backward()\n",
    "            # 对两个optimizer实例例分别调⽤用step函数，从⽽而分别更更新权重和偏差\n",
    "            optimizer_w.step()\n",
    "            optimizer_b.step()\n",
    "        train_ls.append(loss(net(train_features),train_labels).mean().item())\n",
    "        test_ls.append(loss(net(test_features),test_labels).mean().item())\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs','loss',range(1, num_epochs + 1), test_ls, ['train','test'])\n",
    "    print('L2 norm of w:', net.weight.data.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#丢弃法\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import d2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X,drop_prob):\n",
    "    X=X.float()\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob=1-drop_prob\n",
    "    if keep_prob==0:\n",
    "        return torch.zeros_like(X)\n",
    "    randx=torch.rand(X.shape)\n",
    "    print(randx)\n",
    "    mask=(randx<keep_prob).float()\n",
    "    print(mask)\n",
    "    return mask*X/keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3748, 0.9882, 0.7937, 0.8838, 0.3302, 0.0589, 0.7055, 0.4423],\n",
      "        [0.3338, 0.6837, 0.0489, 0.4526, 0.4407, 0.9516, 0.4510, 0.1424]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1.]])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0., 25.,  0.,  0.],\n",
      "        [ 0.,  0., 50.,  0.,  0.,  0.,  0., 75.]])\n"
     ]
    }
   ],
   "source": [
    "X=torch.arange(16).view(2,8)\n",
    "print(dropout(X,0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型构造\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量归一化\n",
    "import time \n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import sys\n",
    "import d2p\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def batch_norm(is_training,X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not is_training:\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape)==2:\n",
    "            mean=X.mean(dim=0)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2,keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0,keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            # 训练模式下⽤用当前的均值和⽅方差做标准化\n",
    "            X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "            # 更更新移动平均的均值和⽅方差\n",
    "            moving_mean = momentum * moving_mean + (1.0 - momentum) *mean\n",
    "            moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BatchNorm层\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        if num_dims==2:#全连接层\n",
    "            shape=(1,num_features)\n",
    "        else:\n",
    "            shape=(1,num_featuresq,1,1)\n",
    "        self.gamma=nn.Parameter(torch.ones(shape))\n",
    "        self.beta=nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean=torch.zeros(shape)\n",
    "        self.moving_var=torch.zeros(shape)\n",
    "    def forward(self,X):\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean=self.moving_mean.to(X.device)\n",
    "            self.moving_var=self.moving_var.to(X.device)\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(self.training,\n",
    "                                                     X,self.gamma,self.beta,self.moving_mean,\n",
    "                                                     selfl.moving_var,eps=1e-5,momentuum=0.9)\n",
    "        return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
