看了一遍我竟然没找到创新点...就只是再低层添加了监督学习吗？论文只有5页，ACL不是NLP的顶会吗...  
# <div align=center>Deep multi-task learning with low level tasks supervised at lower layers</div>  

## 摘要  
提出了一种具有深度双向RNNs的多任务学习结构，不同的任务可以在不同的层次上进行监控。结果表明，在系统的最内层而非最外层进行位置监控效果更好。这是因为低层任务更好地保存在较低的层中，从而使较高层任务能够利用低层任务的共享表示。  
## 1 引言  
贡献是一个新颖的见解，即(历史上被认为是)低层次的任务在这样一个架构的低层次中被更好地建模。在NLP中，很自然地把某些层次的分析看作是对其他层次的补充，通常是低层次任务对高层次任务的补充。因此，本文在低层的监督通常使用的是低级任务。本文的主要贡献：提出了一种用于序列标记的多任务学习的深度双向RNN；证明了在表层的监督学习通常是次优的；此种结构适用于领域自适应。  
## 2 深度双向RNN用于序列标注  
符号定义，双向RNN，深度双向RNN，序列标记，多任务深度双向RNN，在不同的层监督不同的任务，形成级联任务网络。  
所以说，本文创新点就是这种想法？  
## 3 实验  
NER没有改进，POS、Chunk、CCG有改进，证明只有任务足够相似的情况下才会有改进。  
做了领域自适应实验（即数据为来自不同的来源的文本），有效果。  
## 4 总结  
我们怀疑在不同的任务之间存在一个层次结构，我们证明它是值得的，同时把这个知识合并到MTL体系结构的设计中，通过使较低层次的任务影响较低层的表示。  
**这就完事了。**