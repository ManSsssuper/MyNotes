#### 1、最大熵模型通俗理解  
https://www.jianshu.com/p/a8ed01aeefc9  
#### 2、logistic回归和NB的关系  
https://www.zhihu.com/question/265995680  
#### 3、将LR与adaboost组合  
https://blog.csdn.net/mgl8887/article/details/93983195  
#### 4、LR与SVM比较  
LR和SVM都可以处理分类问题，且一般都处理线性二分类问题。
2. LR是参数模型，SVM是非参数模型。
3. LR的目标函数是对数似然函数，SVM的目标函数是hinge损失函数。这两个函数都是增加对分类结果影响较大的数据点的权重，减少影响较小的数据点的权重。
4. SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
5. 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
6. logic能做的svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。 
#### 5、LR与最大熵模型的关系  
最大熵模型可以退化为logistic回归模型。首先限定y是一个二元变量，不失一般性，假设取值为0,1.即y∈{0,1}，再定义关于x，y的特征函数为
$$f(x,y)=\Bigg\{{g(x),y=1\atop0,y=0}$$
它与最大熵模型特征函数的区别在于，最大熵模型特征函数是限定了x,y之间的关系，而这里显然没有。将其带入最大熵模型
$$P(y=1|x)=\frac{\exp(w⋅f(x,1))}{(w⋅f(x,0))+exp(w⋅f(x,1))}$$
$$=\frac{exp(w⋅g(x)}{exp(0)+exp(w⋅g(x))}$$
$$=\frac{1}{exp(−w⋅g(x))+1}$$
当g(x)为sigmoid函数时，这就是logistic回归模型。
同理可计算P(y=0|x)。
