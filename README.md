# Papers
<table>
  <tr>
      <td>内容</td>
      <td>类型</td>
      <td>开始时间</td>
      <td>截止时间</td>
      <td>备注</td>
  </tr>
  <tr>
      <td>Deep-MTL方法型论文</td>
      <td>方法</td>
      <td> 9/17/2019 9:56:31 PM </td>
      <td> 10/24/2019 3:29:15 PM </td>
      <td>18篇论文</td>
  </tr>
  <tr>
      <td>深度学习基础-花书</td>
      <td>知识</td>
      <td> 10/24/2019 3:29:15 PM </td>
      <td> 10/29/2019 7:35:10 PM </td>
      <td>线代、概率论、数值计算</td>
  </tr>
  <tr>
      <td>Deep-MTL应用型论文</td>
      <td>应用</td>
      <td> 10/29/2019 7:36:01 PM </td>
      <td> 10/30/2019 7:36:01 PM </td>
      <td>ICML的论文，侧重NLP和CV</td>
  </tr>
  <tr>
      <td>play and never play</td>
      <td>ss</td>
      <td> 10/30/2019 7:36:01 PM </td>
      <td> 11/03/2019 12:00:00 PM </td>
      <td> Never until have one paper </td>
  </tr>
  <tr>
      <td>Deep-MTL应用型论文</td>
      <td>应用</td>
      <td>11/4/2019 10:02:48 AM </td>
      <td> 11/5/2019 2:39:54 PM  </td>
      <td>-</td>
  </tr>
  <tr>
      <td>Deep-Multi-Task with Two-level attention</td>
      <td>应用</td>
      <td>11/5/2019 2:40:38 PM </td>
      <td> 11/5/2019 3:41:51 PM  </td>
      <td>确定与想法无关</td>
  </tr>
  <tr>
      <td>重读Attention三篇论文</td>
      <td>方法</td>
      <td>11/5/2019 2:40:38 PM </td>
      <td> - </td>
      <td>《Multiple_Relational_Attention_Network_for_Multi-task_Learning》《Modeling_Task_Relationships_in_MTL_with_Multi-gate_Mixture-of-Experts》《Attention is all you need》</td>
  </tr>
</table>
